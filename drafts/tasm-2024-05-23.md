## News

Tyler Cowen declares AI safety movement dead. [Manifold currently disagrees](https://manifold.markets/ZviMowshowitz/will-may-2024-be-remembered-as-the) (at time of writing, that was at 4%).

Q: Is there a lot of overlap between safety and capabilities research?
A: There's a lot of overlap in terms of the required skillset (math, stats, tech), but the outputs can be quite different.

- AI Seoul Summit got commitments from Amazon, Anthropic, Cohere, Google, G42, IBM, Inflection AI, Meta, Microsoft, Mistral AI, Naver, OpenAI, Samsung, Technology Innovation Institute, xAI and Zhipu.ai about assessing the risks posed by their AI outputs (specifically, they're setting out policies/processes they currently intend to follow, but don't explicitly commit to following them once they're established).
- Mapping the Mind of a Large Language Model; anthropic published a [cool new report](https://www.anthropic.com/news/mapping-mind-language-model) on advances in interpretability
  - Apparently Golden Gate Bridge Claude is available now, if you're a subscriber?
  - Also, [this](https://transformer-circuits.pub/2024/scaling-monosemanticity/) is somewhat related
- Scarlett Johansson is suing OpenAI, which seems pretty hardcore. And also, she has a pretty good case.
- Confirmed that Illya Sutskever and Jan Leike have left OpenAI, and there's been a bunch of related fallout/speculation. Which is like. I don't know, man. Read [the zvi](https://thezvi.wordpress.com/2024/05/20/openai-exodus/) for details, but seem pretty unequivocally bad and I'm not in favor.

## The Talk - Possible Futures

### What We're Not Talking Explicitly About

1. Robin Hanson's "em world" (as described in Age of Em)
2. Fast takeoff "foom" scenarios (settles into a superintelligent, goal-directed singleton)
  - Nora Belrose talk is mentioned here, but I'm not familiar with it.
3. Transhuman perspective our minds merging with the AI somehow
4. AIs are just like us (basically, there's just another tribe; this sounds like Matrix world?)
5. Perfectly balanced competing intelligences
6. Human extinction scenarios (except very briefly)

### "GPT-5 Only" World

- The year is 2044, and scaling laws didn't work. AI capabilities plateau at a level we'd recognize as just a bit above what we have in mid-2024. Further progress would require some kind of theoretical breakthrough (which might happen in the deep future, but not in anyone's lifespan today).
- Very multimodal
- Technological and economic flow-through of the current AI boom
- Effect on GDP
  - Gonna be complicated. Tech innovations can bolster GDP growth. Under "business as usual", world GDP per capita would grow around 50% over the next 20ish years.
- Gary Marcus is apparently bearish on AI progress, and so is the natural avatar of this view
- More tasks get automated, but distribution won't be equal across all jobs. If your job is partially automated, that's great because it'll let you get things done, if it's fully automated, it kind of sucks for you under the current economic system because you'll get fired and replaced with robots.
- GDP increase could make UBI or similar more feasible
- "Inequality" is hard to think about here because there are basically three separate measures you might want to consider: income inequality, wealth inequality and consumption inequality. They have different implications and different prescriptions.

### "Totalitarian" World

The most powerful entities - in particular governments of large countries - benefit the most from advances. Not necessarily a single world government, so this isn't a singleton, but the state gets proportionally more capability.
- Surveillance/data collection is one dimension that might be affected here
- Surveillance analysis:
  - Multimodal models + key off a person's identity. Build up a profile of each person and their activities.
  - Predicting behavior; anomaly detection (if you go outside your normal routine, it might be subject to more detailed monitoring)
  - Totalitarian governments constranied by AI ethics or fairness
- Another suggestion: super efficient/effective personalized propaganda
- A government's primary goal in this world would be to maintain its own stability 

### Bonus

Tegmark's 12 AI aftermath scenario chart. This was a chart that the organizer put together summarizing [this article](https://futureoflife.org/ai/ai-aftermath-scenarios/). It was pretty interesting, and you'll be able to find it on the presentation slides once they get published.
